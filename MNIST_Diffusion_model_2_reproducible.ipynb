{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chwon9-jpg/Diffusion_models/blob/main/MNIST_Diffusion_model_2_reproducible.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvRcoKHxhWf-"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Required commands\n",
        "# ----------------------------\n",
        "\n",
        "!pip install deepinv\n",
        "!pip install pandas\n",
        "!pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Required libraries\n",
        "# ----------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "import torchvision.transforms as transforms, torchvision, matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "FBy1u7tq92wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shIwo2I-B0uO"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# U-Net architecture\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Sinusoidal Positional Embedding used in Transformers (used for timestep embedding)\n",
        "# ----------------------------\n",
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None].float() * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Self-Attention Block\n",
        "# ----------------------------\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.proj_query = nn.Conv2d(channels, channels // 8, kernel_size=1)\n",
        "        self.proj_key = nn.Conv2d(channels, channels // 8, kernel_size=1)\n",
        "        self.proj_value = nn.Conv2d(channels, channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        # 1x1 convolutional layers act as linear transformations\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()  # Batch_size, Channels, Height, Width\n",
        "\n",
        "        query = self.proj_query(x).view(B, -1, H * W).permute(0, 2, 1)  # B, HW, C/8\n",
        "        key = self.proj_key(x).view(B, -1, H * W)                      # B, C/8, HW\n",
        "\n",
        "        energy = torch.bmm(query, key)\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "\n",
        "        value = self.proj_value(x).view(B, -1, H * W)                   # B, C, HW\n",
        "\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1)) # B, C, HW\n",
        "        out = out.view(B, C, H, W)\n",
        "\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Simple UNet for MNIST Diffusion\n",
        "# ----------------------------\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=1, hidden_dim=32):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Conv2d(in_channels, hidden_dim, 3, padding=1)\n",
        "\n",
        "        self.enc2 = nn.Conv2d(hidden_dim, hidden_dim * 2, 3, padding=1)\n",
        "\n",
        "        self.enc3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 3, padding=1)\n",
        "\n",
        "        self.enc4 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, 3, padding=1)\n",
        "\n",
        "\n",
        "        # Attention after deepest encoder\n",
        "        self.attn1 = SelfAttention(hidden_dim * 8)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 3, padding=1)\n",
        "\n",
        "        # Attention after bottleneck\n",
        "        self.attn2 = SelfAttention(hidden_dim * 8)\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.dec4 = nn.Conv2d(hidden_dim * 8 + hidden_dim * 4, hidden_dim * 4, 3, padding=1)\n",
        "\n",
        "        self.dec3 = nn.Conv2d(hidden_dim * 4 + hidden_dim * 2, hidden_dim * 2, 3, padding=1)\n",
        "\n",
        "        self.dec2 = nn.Conv2d(hidden_dim * 2 + hidden_dim, hidden_dim, 3, padding=1)\n",
        "\n",
        "        self.attn3 = SelfAttention(hidden_dim)\n",
        "\n",
        "        self.dec1 = nn.Conv2d(hidden_dim + in_channels, out_channels, 3, padding=1)\n",
        "\n",
        "\n",
        "        # Timestep embedding: Using Sinusoidal Positional Embedding + MLP\n",
        "        time_embedding_dim = hidden_dim * 4 # Intermediate dim for sinusoidal embedding\n",
        "        self.sinusoidal_embed = SinusoidalPositionalEmbedding(dim=time_embedding_dim)\n",
        "\n",
        "        self.time_mlp_bottleneck = nn.Sequential(\n",
        "            nn.Linear(time_embedding_dim, hidden_dim * 8), # Project to final bottleneck dim\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim * 8, hidden_dim * 8)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, type_t=\"timestep\"):\n",
        "        # Process timestep with positional embedding and MLP\n",
        "        sinusoidal_emb = self.sinusoidal_embed(t)\n",
        "\n",
        "        t_emb_bottleneck = self.time_mlp_bottleneck(sinusoidal_emb)\n",
        "        t_emb_bottleneck = t_emb_bottleneck.unsqueeze(-1).unsqueeze(-1)  # [B, hidden_dim * 8, 1, 1]\n",
        "\n",
        "        # Encoder (Downsampling)\n",
        "        e1 = F.silu(self.enc1(x))\n",
        "        p1 = F.max_pool2d(e1, 2)\n",
        "        e2 = F.silu(self.enc2(p1))\n",
        "        p2 = F.max_pool2d(e2, 2)\n",
        "        e3 = F.silu(self.enc3(p2))\n",
        "        p3 = F.max_pool2d(e3, 2)\n",
        "        e4 = F.silu(self.enc4(p3))\n",
        "\n",
        "        # Attention 1\n",
        "        e4_attn = self.attn1(e4)\n",
        "\n",
        "        # Bottleneck + timestep (MidBlock)\n",
        "        b = F.silu(self.bottleneck(e4_attn) + t_emb_bottleneck)       # Conv -> Add T_emb -> SiLU\n",
        "        # 4 x 4 spatial dimension\n",
        "\n",
        "        # Attention 2\n",
        "        b_attn = self.attn2(b)\n",
        "\n",
        "        # Decoder (Upsampling)\n",
        "        u4 = F.interpolate(b_attn, scale_factor=2, mode=\"nearest\") # Spatial dimensions are upscaled by a factor of 2\n",
        "        d4 = F.silu(self.dec4(torch.cat([u4, e3], dim=1))) # Skip-connection -> Conv -> SiLU\n",
        "\n",
        "        u3 = F.interpolate(d4, scale_factor=2, mode=\"nearest\")\n",
        "        d3 = F.silu(self.dec3(torch.cat([u3, e2], dim=1))) # Skip-connection -> Conv -> SiLU\n",
        "\n",
        "        u2 = F.interpolate(d3, scale_factor=2, mode=\"nearest\")\n",
        "        d2 = F.silu(self.dec2(torch.cat([u2, e1], dim=1))) # Skip-connection -> Conv -> SiLU\n",
        "\n",
        "        d2_attn = self.attn3(d2)\n",
        "\n",
        "        # Final output (no activation or normalization after the last layer)\n",
        "        out = self.dec1(torch.cat([d2_attn, x], dim=1))\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_BMlhqP2ivS"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience, delta):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_score = None # Track the best validation score\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "        self.best_model_state = None # Track the best model state\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_model_state = model.state_dict()\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.best_model_state = model.state_dict()\n",
        "            self.counter = 0\n",
        "\n",
        "    def load_best_model(self, model):\n",
        "        model.load_state_dict(self.best_model_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwNrnnuKIBFb"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Training Setup\n",
        "# ----------------------------\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "image_size = 32\n",
        "lr = 1e-3\n",
        "epochs = 50\n",
        "timesteps = 500\n",
        "beta_start = 1e-4\n",
        "beta_end = 0.02\n",
        "\n",
        "# Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)), # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "print(\"Loading MNIST dataset...\")\n",
        "full_trainset = torchvision.datasets.MNIST(root='./data',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.8 * len(full_trainset)) # 80% for training\n",
        "val_size = len(full_trainset) - train_size  # Remaining for validation\n",
        "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size, shuffle=False) # Use a separate dataloader for validation dataset\n",
        "\n",
        "print(f\"Dataset loaded. Training batches: {len(trainloader)}, Validation batches: {len(valloader)}\\n\")\n",
        "\n",
        "# Model, optimizer, loss\n",
        "model = SimpleUNet(in_channels=1, out_channels=1, hidden_dim=32).to(device)\n",
        "initial_model_state = {k: v.clone() for k, v in model.state_dict().items()} # Save initial weights\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "mse = nn.MSELoss()\n",
        "early_stopping = EarlyStopping(patience=10, delta=0.001)\n",
        "\n",
        "# Precompute noise schedule\n",
        "betas = torch.linspace(beta_start, beta_end, timesteps, device=device)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "\n",
        "\n",
        "# Print model size\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model: SimpleUNet | Parameters: {num_params:,}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XGhUijtIBCh"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "\n",
        "print(\"Starting training...\\n\")\n",
        "total_start_time = time.time()\n",
        "\n",
        "training_history = [] # Initialize list to store training history\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for data, _ in trainloader:\n",
        "        imgs = data.to(device)\n",
        "        noise = torch.randn_like(imgs)\n",
        "        t = torch.randint(0, timesteps, (imgs.size(0),), device=device)\n",
        "\n",
        "        # Add noise\n",
        "        noised_imgs = (\n",
        "            sqrt_alphas_cumprod[t, None, None, None] * imgs\n",
        "            + sqrt_one_minus_alphas_cumprod[t, None, None, None] * noise\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad() # Avoid Gradient Accumulation\n",
        "        predicted_noise = model(noised_imgs, t) # Predict noise using U-net Model as mentioned before\n",
        "        loss = mse(predicted_noise, noise)\n",
        "        # Backprop + Update model params\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(trainloader)\n",
        "    # average loss that the model incurred on each batch\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for data, _ in valloader:\n",
        "            imgs = data.to(device)\n",
        "            noise = torch.randn_like(imgs)\n",
        "            t = torch.randint(0, timesteps, (imgs.size(0),), device=device)\n",
        "\n",
        "            noised_imgs = (\n",
        "                sqrt_alphas_cumprod[t, None, None, None] * imgs\n",
        "                + sqrt_one_minus_alphas_cumprod[t, None, None, None] * noise\n",
        "            )\n",
        "            predicted_noise = model(noised_imgs, t)\n",
        "            val_loss = mse(predicted_noise, noise)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(valloader)\n",
        "\n",
        "    # Store results for plotting\n",
        "    training_history.append({\n",
        "        'Epoch': epoch + 1,\n",
        "        'Avg Train Loss': avg_train_loss,\n",
        "        'Avg Val Loss': avg_val_loss\n",
        "    })\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    total_elapsed = time.time() - total_start_time\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} | \"\n",
        "          f\"Avg Train Loss: {avg_train_loss:.6f} | \"\n",
        "          f\"Avg Val Loss: {avg_val_loss:.6f} | \"\n",
        "          f\"Epoch Time: {epoch_time:.2f}s | \"\n",
        "          f\"Total Time: {total_elapsed:.2f}s\")\n",
        "\n",
        "    early_stopping(avg_val_loss, model) # Pass the validation loss\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "\n",
        "early_stopping.load_best_model(model) # Load the best model weights found during training\n",
        "final_model_state = model.state_dict() # Save final weights\n",
        "\n",
        "# ----------------------------\n",
        "# Save Model\n",
        "# ----------------------------\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"models/simple_diffusion_model.pth\")\n",
        "print(\"\\nTraining finished!\")\n",
        "print(\"Model saved to models/simple_diffusion_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "708c2fe5"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Loss visualization\n",
        "# ----------------------------\n",
        "\n",
        "# Create DataFrame directly from training_history\n",
        "loss_df = pd.DataFrame(training_history)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='Epoch', y='Avg Train Loss', data=loss_df, label='Average Training Loss')\n",
        "sns.lineplot(x='Epoch', y='Avg Val Loss', data=loss_df, label='Average Validation Loss')\n",
        "plt.title('Average Training and Validation Loss Over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWfO4XAHJ2f6"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Sampling Generation\n",
        "# ----------------------------\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "num_samples = 32\n",
        "img_size = 32\n",
        "\n",
        "# Start with pure noise\n",
        "x_t = torch.randn(num_samples, 1, img_size, img_size, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for t in reversed(range(timesteps)):\n",
        "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
        "\n",
        "        # Predict noise ε_θ(x_t, t)\n",
        "        predicted_noise = model(x_t, t_batch)\n",
        "\n",
        "        # Compute mean and variance for reverse step\n",
        "        alpha_t = alphas[t]\n",
        "        alpha_bar_t = alphas_cumprod[t]\n",
        "        alpha_bar_t_prev = alphas_cumprod[t-1] if t > 0 else torch.tensor(1.0, device=device)\n",
        "        beta_t = betas[t]\n",
        "\n",
        "        # Denoise: x_{t-1} = 1/sqrt(alpha_t) * (x_t - beta_t/sqrt(1-alpha_bar_t) * predicted_noise) + sigma_t * z\n",
        "        x_0_pred = (x_t - sqrt_one_minus_alphas_cumprod[t] * predicted_noise) / sqrt_alphas_cumprod[t]\n",
        "        x_0_pred = torch.clamp(x_0_pred, -1, 1)  # Clip to [-1,1] for stability\n",
        "\n",
        "        mean = x_t - ((beta_t * predicted_noise) / (sqrt_one_minus_alphas_cumprod[t]))\n",
        "        mean = mean / torch.sqrt(alpha_t)\n",
        "\n",
        "        if t == 0:\n",
        "            # final step: don't add noise, just use the predicted x_0\n",
        "            x_t = x_0_pred\n",
        "        else:\n",
        "            variance = (1 - alpha_bar_t_prev) / (1 - alpha_bar_t)\n",
        "            variance = variance * beta_t  # still unused, but kept to match your original\n",
        "            sigma_t = variance ** 0.5\n",
        "            z = torch.randn_like(x_t)  # same shape & device as x_t\n",
        "            x_t = mean + sigma_t * z   # only a single tensor, no tuple\n",
        "\n",
        "\n",
        "# Post-process: convert to [0,1] and detach\n",
        "generated_images = (x_t.clamp(-1, 1) + 1) / 2.0  # From [-1,1] to [0,1]\n",
        "\n",
        "# Plot generated images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "fig, axes = plt.subplots(4, 8, figsize=(12, 12))  # 4 rows, 8 columns\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(num_samples):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(generated_images[i].cpu().squeeze(), cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Generated 32 new MNIST-like digits!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwsuMd3IJ2f8"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Real vs Generated samples comparison\n",
        "# ----------------------------\n",
        "\n",
        "# Show real MNIST for reference\n",
        "real_batch = next(iter(trainloader))[0][:32].cpu()\n",
        "real_batch = (real_batch + 1) / 2.0  # Only if you used Normalize((0.5,), (0.5,))\n",
        "\n",
        "# Plot real vs generated side-by-side\n",
        "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(32):\n",
        "    # Real\n",
        "    axes[i*2].imshow(real_batch[i].squeeze(), cmap='gray')\n",
        "    axes[i*2].axis('off')\n",
        "    # Generated\n",
        "    axes[i*2 + 1].imshow(generated_images[i].cpu().squeeze(), cmap='gray')\n",
        "    axes[i*2 + 1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdoahlaByP-I"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Progressive generation setup\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "num_samples = 32\n",
        "img_size = 32\n",
        "\n",
        "captured_images = {}\n",
        "capture_points = [0, 5, 10, 50, 100, 200, 499] # Define timesteps to capture\n",
        "\n",
        "# Start with pure noise\n",
        "x_t = torch.randn(num_samples, 1, img_size, img_size, device=device)\n",
        "\n",
        "# Capture initial pure noise\n",
        "captured_images['initial_noise'] = ((x_t.clone().cpu().clamp(-1, 1) + 1) / 2.0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for t in reversed(range(timesteps)):\n",
        "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
        "\n",
        "        # Capture intermediate x_t at specified timesteps *before* it's updated\n",
        "        if t in capture_points:\n",
        "            processed_x_t = (x_t.clone().cpu().clamp(-1, 1) + 1) / 2.0\n",
        "            captured_images[t] = processed_x_t\n",
        "\n",
        "        # Predict noise ε_θ(x_t, t)\n",
        "        predicted_noise = model(x_t, t_batch)\n",
        "\n",
        "        # Compute mean and variance for reverse step\n",
        "        alpha_t = alphas[t]\n",
        "        alpha_bar_t = alphas_cumprod[t]\n",
        "        alpha_bar_t_prev = alphas_cumprod[t-1] if t > 0 else torch.tensor(1.0, device=device)\n",
        "        beta_t = betas[t]\n",
        "\n",
        "        # Denoise: x_{t-1} = 1/sqrt(alpha_t) * (x_t - beta_t/sqrt(1-alpha_bar_t) * predicted_noise) + sigma_t * z\n",
        "        x_0_pred = (x_t - sqrt_one_minus_alphas_cumprod[t] * predicted_noise) / sqrt_alphas_cumprod[t]\n",
        "        x_0_pred = torch.clamp(x_0_pred, -1, 1)  # Clip to [-1,1] for stability\n",
        "\n",
        "        mean = x_t - ((beta_t * predicted_noise) / (sqrt_one_minus_alphas_cumprod[t]))\n",
        "        mean = mean / torch.sqrt(alpha_t)\n",
        "\n",
        "        if t == 0:\n",
        "            # final step: don't add noise, just use the predicted x_0\n",
        "            x_t = x_0_pred\n",
        "        else:\n",
        "            variance = (1 - alpha_bar_t_prev) / (1 - alpha_bar_t)\n",
        "            variance = variance * beta_t\n",
        "            sigma_t = variance ** 0.5\n",
        "            z = torch.randn_like(x_t)\n",
        "            x_t = mean + sigma_t * z\n",
        "\n",
        "# Post-process the final generated images: convert to [0,1] and detach\n",
        "generated_images = (x_t.clamp(-1, 1) + 1) / 2.0\n",
        "\n",
        "print(\"Intermediate images (x_t) and final generated images captured.\")\n",
        "print(\"Final generated images stored in 'generated_images'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_-46QJWytXE"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Progressive generation grid\n",
        "# ----------------------------\n",
        "\n",
        "# Get sorted capture points for visualization\n",
        "sorted_capture_keys = sorted([k for k in captured_images.keys() if isinstance(k, int)])\n",
        "sorted_capture_keys.append('initial_noise') # Move initial noise to the end\n",
        "\n",
        "# Determine grid dimensions\n",
        "num_rows = len(sorted_capture_keys)\n",
        "num_cols = num_samples # num_samples is already defined as 32\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 1.5, num_rows * 1.5))\n",
        "\n",
        "for i, t_key in enumerate(sorted_capture_keys):\n",
        "    images_batch = captured_images[t_key]\n",
        "\n",
        "    for j in range(num_samples):\n",
        "        ax = axes[i, j]\n",
        "        ax.imshow(images_batch[j].squeeze(), cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "        if j == 0:\n",
        "            if t_key == 'initial_noise':\n",
        "                ax.set_title(f'Initial Noise', loc='left', color='blue')\n",
        "            else:\n",
        "                ax.set_title(f't={t_key}', loc='left', color='blue')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Visualization of intermediate image generation steps\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}