{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chwon9-jpg/Diffusion_models/blob/main/CIFAR_10_Diffusion_model_2_reproducible.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YvRcoKHxhWf-"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Required command\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "!pip install deepinv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# Required libraries\n",
        "# ----------------------------\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torchvision.transforms as transforms, torchvision, matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, random_split"
      ],
      "metadata": {
        "id": "aTBqkawVwtQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shIwo2I-B0uO"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# U-Net architecture\n",
        "# ----------------------------\n",
        "\n",
        "# ----------------------------\n",
        "# Sinusoidal Positional Embedding used in Transformers (used for timestep embedding)\n",
        "# ----------------------------\n",
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None].float() * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Self-Attention Block\n",
        "# ----------------------------\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.channels = channels\n",
        "        self.proj_query = nn.Conv2d(channels, channels // 8, kernel_size=1)\n",
        "        self.proj_key = nn.Conv2d(channels, channels // 8, kernel_size=1)\n",
        "        self.proj_value = nn.Conv2d(channels, channels, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        # 1x1 convolutional layers act as linear transformations\n",
        "        # Weights are randomly initialized just like the convolution layers used in the encoder/decoder pathway\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()  # Batch_size, Channels, Height, Width\n",
        "\n",
        "        query = self.proj_query(x).view(B, -1, H * W).permute(0, 2, 1)  # B, HW, C/8\n",
        "        # HW flattens the spatial dimensions into a single dimension where each HW in the feature map\n",
        "        # corresponds to an index in this flattened dimension space\n",
        "        # Each image in the batch, we have HW query vectors, each of dimension C/8\n",
        "\n",
        "        key = self.proj_key(x).view(B, -1, H * W)                      # B, C/8, HW\n",
        "        # Each image in the batch, we have C/8 feature maps of dimension HW\n",
        "\n",
        "        \"\"\"\n",
        "        query groups features by spatial location WHEREAS key groups features by channel\n",
        "        \"\"\"\n",
        "\n",
        "        energy = torch.bmm(query, key)  # Transposed B, HW,\n",
        "        # batch matrix multiplication, energy = attention scores,\n",
        "        # indicating how much each position should attend to every other position\n",
        "\n",
        "        attention = F.softmax(energy, dim=-1)\n",
        "        # transforms a vector of numbers into a probability distribution\n",
        "        # where each value represents the likelihood of a particular class.\n",
        "\n",
        "        value = self.proj_value(x).view(B, -1, H * W)                   # B, C, HW\n",
        "\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1)) # B, C, HW\n",
        "        out = out.view(B, C, H, W)\n",
        "\n",
        "        out = self.gamma * out + x # + x is the residual connection (skip-connection)\n",
        "        # helps prevent vanishing gradient problem\n",
        "        return out\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Simple UNet for MNIST Diffusion\n",
        "# ----------------------------\n",
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=3, hidden_dim=32):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Conv2d(in_channels, hidden_dim, 3, padding=1)\n",
        "        self.norm_enc1 = nn.GroupNorm(8, hidden_dim)\n",
        "\n",
        "        self.enc2 = nn.Conv2d(hidden_dim, hidden_dim * 2, 3, padding=1)\n",
        "        self.norm_enc2 = nn.GroupNorm(8, hidden_dim * 2)\n",
        "        self.attn0_5 = SelfAttention(hidden_dim * 2)\n",
        "\n",
        "        self.enc3 = nn.Conv2d(hidden_dim * 2, hidden_dim * 4, 3, padding=1)\n",
        "        self.norm_enc3 = nn.GroupNorm(8, hidden_dim * 4)\n",
        "        self.attn0 = SelfAttention(hidden_dim * 4)\n",
        "\n",
        "        self.enc4 = nn.Conv2d(hidden_dim * 4, hidden_dim * 8, 3, padding=1)\n",
        "        self.norm_enc4 = nn.GroupNorm(8, hidden_dim * 8)\n",
        "\n",
        "\n",
        "        # Attention after deepest encoder\n",
        "        self.attn1 = SelfAttention(hidden_dim * 8)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = nn.Conv2d(hidden_dim * 8, hidden_dim * 8, 3, padding=1)\n",
        "        self.norm_bottleneck = nn.GroupNorm(8, hidden_dim * 8)\n",
        "\n",
        "        # Attention after bottleneck\n",
        "        self.attn2 = SelfAttention(hidden_dim * 8)\n",
        "\n",
        "\n",
        "        # Decoder\n",
        "        self.dec4 = nn.Conv2d(hidden_dim * 8 + hidden_dim * 4, hidden_dim * 4, 3, padding=1)\n",
        "        self.norm_dec4 = nn.GroupNorm(8, hidden_dim * 4)\n",
        "\n",
        "        self.dec3 = nn.Conv2d(hidden_dim * 4 + hidden_dim * 2, hidden_dim * 2, 3, padding=1)\n",
        "        self.norm_dec3 = nn.GroupNorm(8, hidden_dim * 2)\n",
        "\n",
        "        self.dec2 = nn.Conv2d(hidden_dim * 2 + hidden_dim, hidden_dim, 3, padding=1)\n",
        "        self.norm_dec2 = nn.GroupNorm(8, hidden_dim)\n",
        "\n",
        "        self.attn3 = SelfAttention(hidden_dim)\n",
        "\n",
        "        self.dec1 = nn.Conv2d(hidden_dim + in_channels, out_channels, 3, padding=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Timestep embedding: Using Sinusoidal Positional Embedding + MLP\n",
        "        time_embedding_dim = hidden_dim * 4 # Intermediate dim for sinusoidal embedding\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            SinusoidalPositionalEmbedding(dim=time_embedding_dim),\n",
        "            nn.Linear(time_embedding_dim, hidden_dim * 8), # Project to final bottleneck dim\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(hidden_dim * 8, hidden_dim * 8)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, type_t=\"timestep\"):\n",
        "        # Process timestep with positional embedding and MLP\n",
        "        t_emb = self.time_mlp(t) # Output: [B, hidden_dim * 8]\n",
        "        # Unsqueeze to match the spatial dimensions of the feature maps\n",
        "        t_emb = t_emb.unsqueeze(-1).unsqueeze(-1)  # [B, hidden_dim * 8, 1, 1]\n",
        "\n",
        "        # Encoder (Downsampling)\n",
        "        e1 = F.silu(self.norm_enc1(self.enc1(x)))                     # Conv -> Norm -> SiLU\n",
        "        p1 = F.max_pool2d(e1, 2) # both kernel filter size and stride are 2\n",
        "\n",
        "        e2_block_out = F.silu(self.norm_enc2(self.enc2(p1)))                   # Conv -> Norm -> SiLU\n",
        "\n",
        "        e2_attn_out = self.attn0_5(e2_block_out)          # Self-Attention -1\n",
        "\n",
        "        p2 = F.max_pool2d(e2_block_out, 2) # Pool applied before attention output, as in original\n",
        "\n",
        "        e3_block_out = F.silu(self.norm_enc3(self.enc3(p2)))                   # Conv -> Norm -> SiLU\n",
        "\n",
        "        e3_attn_out = self.attn0(e3_block_out)                      # Self-Attention 0\n",
        "\n",
        "        p3 = F.max_pool2d(e3_attn_out, 2)\n",
        "\n",
        "        e4_block_out = F.silu(self.norm_enc4(self.enc4(p3)))                   # Conv -> Norm -> SiLU\n",
        "        # Self - Attention 1\n",
        "        e4_attn_out = self.attn1(e4_block_out)\n",
        "\n",
        "        # Bottleneck + timestep (MidBlock)\n",
        "        b_block_out = F.silu(self.norm_bottleneck(self.bottleneck(e4_attn_out) + t_emb))       # Conv -> Add T_emb -> Norm -> SiLU\n",
        "        # 4 x 4 spatial dimension\n",
        "\n",
        "        # Self - Attention 2\n",
        "        b_attn_out = self.attn2(b_block_out)\n",
        "\n",
        "        # Decoder (Upsampling)\n",
        "        u4 = F.interpolate(b_attn_out, scale_factor=2, mode=\"nearest\") # Spatial dimensions are upscaled by a factor of 2\n",
        "        # Skip-connection uses e3_block_out (output of enc3 before its attention or pooling)\n",
        "        d4 = F.silu(self.norm_dec4(self.dec4(torch.cat([u4, e3_block_out], dim=1)))) # Skip-connection -> Conv -> Norm -> SiLU\n",
        "\n",
        "        u3 = F.interpolate(d4, scale_factor=2, mode=\"nearest\")\n",
        "        # Skip-connection uses e2_block_out (output of enc2 before its attention or pooling)\n",
        "        d3 = F.silu(self.norm_dec3(self.dec3(torch.cat([u3, e2_block_out], dim=1)))) # Skip-connection -> Conv -> Norm -> SiLU\n",
        "\n",
        "        u2 = F.interpolate(d3, scale_factor=2, mode=\"nearest\")\n",
        "        # Skip-connection uses e1 (output of enc1 before pooling)\n",
        "        d2_block_out = F.silu(self.norm_dec2(self.dec2(torch.cat([u2, e1], dim=1)))) # Skip-connection -> Conv -> Norm -> SiLU\n",
        "\n",
        "        d2_attn_out = self.attn3(d2_block_out)\n",
        "\n",
        "\n",
        "        # Final output (no activation or normalization after the last layer)\n",
        "        out = self.dec1(torch.cat([d2_attn_out, x], dim=1))\n",
        "\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz1KmqVhCujd"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience, delta):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_score = None # Track the best validation score\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "        self.best_model_state = None # Track the best model state\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.best_model_state = model.state_dict()\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.best_model_state = model.state_dict()\n",
        "            self.counter = 0\n",
        "\n",
        "    def load_best_model(self, model):\n",
        "        model.load_state_dict(self.best_model_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwNrnnuKIBFb"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Setup\n",
        "# ----------------------------\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "image_size = 32\n",
        "lr = 1e-3\n",
        "epochs = 60\n",
        "timesteps = 1000\n",
        "beta_start = 1e-4\n",
        "beta_end = 0.02\n",
        "\n",
        "# Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)), # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "print(\"Loading CIFAR-10 dataset...\")\n",
        "full_trainset = torchvision.datasets.CIFAR10(root='./data',\n",
        "                                        train=True,\n",
        "                                        download=True,\n",
        "                                        transform=transform)\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.8 * len(full_trainset)) # 80% for training\n",
        "val_size = len(full_trainset) - train_size  # Remaining for validation\n",
        "trainset, valset = random_split(full_trainset, [train_size, val_size])\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size, shuffle=True)\n",
        "valloader = DataLoader(valset, batch_size, shuffle=False) # Use a separate dataloader for validation dataset\n",
        "\n",
        "print(f\"Dataset loaded. Training batches: {len(trainloader)}, Validation batches: {len(valloader)}\\n\")\n",
        "\n",
        "# Model, optimizer, loss\n",
        "model = SimpleUNet(in_channels=3, out_channels=3, hidden_dim=32).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "mse = nn.MSELoss()\n",
        "early_stopping = EarlyStopping(patience=10, delta=0.001) # Added delta for a small change threshold\n",
        "# Early stopping is triggered if for 5 consecutive epochs, the improvement is not at least 0.001\n",
        "# compared to the best validation loss recorded so far.\n",
        "\n",
        "# Precompute noise schedule\n",
        "betas = torch.linspace(beta_start, beta_end, timesteps, device=device)\n",
        "alphas = 1.0 - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - alphas_cumprod)\n",
        "\n",
        "\n",
        "# Print model size\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Model: SimpleUNet | Parameters: {num_params:,}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XGhUijtIBCh"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Training Loop\n",
        "# ----------------------------\n",
        "print(\"Starting training...\\n\")\n",
        "total_start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    model.train()\n",
        "    total_train_loss = 0.0\n",
        "\n",
        "    for data, _ in trainloader:\n",
        "        imgs = data.to(device)\n",
        "        noise = torch.randn_like(imgs)\n",
        "        t = torch.randint(0, timesteps, (imgs.size(0),), device=device)\n",
        "\n",
        "        # Add noise\n",
        "        noised_imgs = (\n",
        "            sqrt_alphas_cumprod[t, None, None, None] * imgs\n",
        "            + sqrt_one_minus_alphas_cumprod[t, None, None, None] * noise\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad() # Avoid Gradient Accumulation\n",
        "        predicted_noise = model(noised_imgs, t) # Predict noise using U-net Model as mentioned before\n",
        "        loss = mse(predicted_noise, noise)\n",
        "        # Backprop + Update model params\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(trainloader)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for data, _ in valloader:\n",
        "            imgs = data.to(device)\n",
        "            noise = torch.randn_like(imgs)\n",
        "            t = torch.randint(0, timesteps, (imgs.size(0),), device=device)\n",
        "\n",
        "            noised_imgs = (\n",
        "                sqrt_alphas_cumprod[t, None, None, None] * imgs\n",
        "                + sqrt_one_minus_alphas_cumprod[t, None, None, None] * noise\n",
        "            )\n",
        "            predicted_noise = model(noised_imgs, t)\n",
        "            val_loss = mse(predicted_noise, noise)\n",
        "            total_val_loss += val_loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(valloader)\n",
        "\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "    total_elapsed = time.time() - total_start_time\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs} | \"\n",
        "          f\"Avg Train Loss: {avg_train_loss:.6f} | \"\n",
        "          f\"Avg Val Loss: {avg_val_loss:.6f} | \"\n",
        "          f\"Epoch Time: {epoch_time:.2f}s | \"\n",
        "          f\"Total Time: {total_elapsed:.2f}s\")\n",
        "\n",
        "    early_stopping(avg_val_loss, model) # Pass the validation loss\n",
        "    if early_stopping.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n",
        "\n",
        "\n",
        "early_stopping.load_best_model(model) # Load the best model weights found during training\n",
        "\n",
        "# ----------------------------\n",
        "# Save Model\n",
        "# ----------------------------\n",
        "\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "torch.save(model.state_dict(), \"models/simple_diffusion_model.pth\")\n",
        "print(\"\\nTraining finished!\")\n",
        "print(\"Model saved to models/simple_diffusion_model.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWfO4XAHJ2f6"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Sampling Generation\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "model.eval()  # Set to evaluation mode\n",
        "num_samples = 32\n",
        "img_size = 32\n",
        "\n",
        "# Start with pure noise\n",
        "x_t = torch.randn(num_samples, 3, img_size, img_size, device=device) # Changed to 3 channels\n",
        "\n",
        "with torch.no_grad():\n",
        "    for t in reversed(range(timesteps)):\n",
        "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
        "\n",
        "        # Predict noise ε_θ(x_t, t)\n",
        "        predicted_noise = model(x_t, t_batch)\n",
        "\n",
        "        # Compute mean and variance for reverse step\n",
        "        alpha_t = alphas[t]\n",
        "        alpha_bar_t = alphas_cumprod[t]\n",
        "        alpha_bar_t_prev = alphas_cumprod[t-1] if t > 0 else torch.tensor(1.0, device=device)\n",
        "        beta_t = betas[t]\n",
        "\n",
        "        # Denoise: x_{t-1} = 1/sqrt(alpha_t) * (x_t - beta_t/sqrt(1-alpha_bar_t) * predicted_noise) + sigma_t * z\n",
        "        x_0_pred = (x_t - sqrt_one_minus_alphas_cumprod[t] * predicted_noise) / sqrt_alphas_cumprod[t]\n",
        "        x_0_pred = torch.clamp(x_0_pred, -1, 1)  # Clip to [-1,1] for stability\n",
        "\n",
        "        mean = x_t - ((beta_t * predicted_noise) / (sqrt_one_minus_alphas_cumprod[t]))\n",
        "        mean = mean / torch.sqrt(alpha_t)\n",
        "\n",
        "        if t == 0:\n",
        "            # final step: don't add noise, just use the predicted x_0\n",
        "            x_t = x_0_pred\n",
        "        else:\n",
        "            variance = (1 - alpha_bar_t_prev) / (1 - alpha_bar_t)\n",
        "            variance = variance * beta_t  # still unused, but kept to match your original\n",
        "            sigma_t = variance ** 0.5\n",
        "            z = torch.randn_like(x_t)  # same shape & device as x_t\n",
        "            x_t = mean + sigma_t * z   # only a single tensor, no tuple\n",
        "\n",
        "\n",
        "# Post-process: convert to [0,1] and detach\n",
        "generated_images = (x_t.clamp(-1, 1) + 1) / 2.0  # From [-1,1] to [0,1]\n",
        "\n",
        "# Plot generated images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "fig, axes = plt.subplots(4, 8, figsize=(12, 12))  # 4 rows, 8 columns\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(num_samples):\n",
        "    ax = axes[i]\n",
        "    ax.imshow(generated_images[i].cpu().permute(1, 2, 0)) # Permute for matplotlib, removed cmap='gray'\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Generated 32 new CIFAR-10-like images!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwsuMd3IJ2f8"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Real vs Generated samples comparison\n",
        "# ----------------------\n",
        "\n",
        "\n",
        "# Show real CIFAR-10 for reference\n",
        "real_batch = next(iter(trainloader))[0][:32].cpu() # Changed train_loader to trainloader\n",
        "real_batch = (real_batch + 1) / 2.0  # Only if you used Normalize((0.5,), (0.5,))\n",
        "\n",
        "# Plot real vs generated side-by-side\n",
        "fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(32):\n",
        "    # Real\n",
        "    ax = axes[i*2]\n",
        "    ax.imshow(real_batch[i].permute(1, 2, 0)) # Permute for matplotlib, removed cmap='gray'\n",
        "    ax.axis('off')\n",
        "    # Generated\n",
        "    ax = axes[i*2 + 1]\n",
        "    ax.imshow(generated_images[i].cpu().permute(1, 2, 0)) # Permute for matplotlib, removed cmap='gray'\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}